{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zvAW09stHfK",
        "outputId": "724ee64b-7f36-4479-cab2-3c9ffd9cbff0"
      },
      "source": [
        "!pip install keras_metrics\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import keras_metrics # for recall and precision metrics\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras_metrics) (2.8.0)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iSv5KEWtQQz"
      },
      "source": [
        "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
        "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
        "TEST_SIZE = 0.25 # ratio of testing set\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20 # number of epochs\n",
        "\n",
        "# to convert labels to integers and vice-versa\n",
        "label2int = {\"ham\": 0, \"spam\": 1}\n",
        "int2label = {0: \"ham\", 1: \"spam\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1yCIj9jtjEa"
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads SMS Spam Collection dataset\n",
        "    \"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(\"/content/drive/MyDrive/SOAI/Data/SMSSpamCollection\") as f:\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            labels.append(split[0].strip())\n",
        "            texts.append(' '.join(split[1:]).strip())\n",
        "    return texts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh9vtiGBtrQB"
      },
      "source": [
        "X, y = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV8AT8BQum8r"
      },
      "source": [
        "# Text tokenization\n",
        "# vectorizing text, turning each text into sequence of integers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "# convert to sequence of integers\n",
        "X = tokenizer.texts_to_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvsBOOdGvBdx",
        "outputId": "bf4702c0-61d0-4528-93a8-536d285daa98"
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[49, 471, 4435, 842, 755, 658, 64, 8, 1327, 88, 123, 351, 1328, 148, 2996, 1329, 67, 58, 4436, 144]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMJ1e9jlvDuV",
        "outputId": "4b65acc0-9b09-4395-fd40-2dfb6b9606d9"
      },
      "source": [
        "# convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# pad sequences at the beginning of each sequence with 0's\n",
        "# for example if SEQUENCE_LENGTH=4:\n",
        "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
        "# will be transformed to:\n",
        "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
        "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be6OcUTWvIiY"
      },
      "source": [
        "# One Hot encoding labels\n",
        "# [spam, ham, spam, ham, ham] will be converted to:\n",
        "# [1, 0, 1, 0, 1] and then to:\n",
        "# [[0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
        "\n",
        "y = [label2int[label] for label in y]\n",
        "y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk20xy1gvKLh",
        "outputId": "e676f10a-08f8-4e39-a35c-42a19a4096be"
      },
      "source": [
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcD0gHQTvL9a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "391lwmtzvSkS"
      },
      "source": [
        "def get_model(tokenizer, lstm_units):\n",
        "    \"\"\"\n",
        "    Constructs the model,\n",
        "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
        "    \"\"\"\n",
        "    # get the GloVe embedding vectors\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
        "              EMBEDDING_SIZE,\n",
        "              trainable=False,\n",
        "              input_length=SEQUENCE_LENGTH))\n",
        "\n",
        "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    # compile as rmsprop optimizer\n",
        "    # aswell as with recall metric\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYR52WwWvcY5",
        "outputId": "fab39f8f-18c1-4263-b0fb-566b70b8b939"
      },
      "source": [
        "# constructs the model with 128 LSTM units\n",
        "model = get_model(tokenizer=tokenizer, lstm_units=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          901000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               117248    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,018,506\n",
            "Trainable params: 117,506\n",
            "Non-trainable params: 901,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDK6NlNvh7h",
        "outputId": "cbdd9cc1-d403-424f-9beb-308fdaed4598"
      },
      "source": [
        "# constructs the model with 128 LSTM units\n",
        "model = get_model(tokenizer=tokenizer, lstm_units=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          901000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               117248    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,018,506\n",
            "Trainable params: 117,506\n",
            "Non-trainable params: 901,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6xN002SxB-i",
        "outputId": "1f192573-1fbd-41b0-e075-0b91449f6346"
      },
      "source": [
        "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
        "# model checkpoint for saving best weights\n",
        "model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}\", save_best_only=True,\n",
        "                                    verbose=1)\n",
        "# for better visualization\n",
        "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape: (4180, 100)\n",
            "X_test.shape: (1394, 100)\n",
            "y_train.shape: (4180, 2)\n",
            "y_test.shape: (1394, 2)\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_metrics/metrics.py:26: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return object.__getattribute__(self, name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.8641 - precision: 0.8585 - recall: 0.9556WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n",
            "66/66 [==============================] - 34s 447ms/step - loss: 0.3358 - accuracy: 0.8641 - precision: 0.8585 - recall: 0.9556 - val_loss: 0.2962 - val_accuracy: 0.8594 - val_precision: 0.8720 - val_recall: 0.9880\n",
            "Epoch 2/20\n",
            "66/66 [==============================] - 21s 312ms/step - loss: 0.2701 - accuracy: 0.8778 - precision: 0.8751 - recall: 0.9876 - val_loss: 0.2545 - val_accuracy: 0.8759 - val_precision: 0.8790 - val_recall: 0.9858\n",
            "Epoch 3/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.2316 - accuracy: 0.9086 - precision: 0.8835 - recall: 0.9861 - val_loss: 0.2978 - val_accuracy: 0.8780 - val_precision: 0.8886 - val_recall: 0.9858\n",
            "Epoch 4/20\n",
            "66/66 [==============================] - 20s 309ms/step - loss: 0.2043 - accuracy: 0.9227 - precision: 0.8933 - recall: 0.9857 - val_loss: 0.2004 - val_accuracy: 0.9354 - val_precision: 0.8986 - val_recall: 0.9846\n",
            "Epoch 5/20\n",
            "66/66 [==============================] - 21s 321ms/step - loss: 0.1832 - accuracy: 0.9347 - precision: 0.9034 - recall: 0.9841 - val_loss: 0.2171 - val_accuracy: 0.9082 - val_precision: 0.9068 - val_recall: 0.9845\n",
            "Epoch 6/20\n",
            "66/66 [==============================] - 20s 309ms/step - loss: 0.1729 - accuracy: 0.9364 - precision: 0.9092 - recall: 0.9846 - val_loss: 0.3217 - val_accuracy: 0.8802 - val_precision: 0.9110 - val_recall: 0.9848\n",
            "Epoch 7/20\n",
            "66/66 [==============================] - 21s 312ms/step - loss: 0.1752 - accuracy: 0.9388 - precision: 0.9123 - recall: 0.9852 - val_loss: 0.1959 - val_accuracy: 0.9175 - val_precision: 0.9142 - val_recall: 0.9854\n",
            "Epoch 8/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1541 - accuracy: 0.9457 - precision: 0.9158 - recall: 0.9858 - val_loss: 0.3909 - val_accuracy: 0.8845 - val_precision: 0.9184 - val_recall: 0.9840\n",
            "Epoch 9/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1630 - accuracy: 0.9462 - precision: 0.9208 - recall: 0.9826 - val_loss: 0.1889 - val_accuracy: 0.9197 - val_precision: 0.9223 - val_recall: 0.9828\n",
            "Epoch 10/20\n",
            "66/66 [==============================] - 20s 311ms/step - loss: 0.1439 - accuracy: 0.9478 - precision: 0.9235 - recall: 0.9830 - val_loss: 0.2418 - val_accuracy: 0.9118 - val_precision: 0.9252 - val_recall: 0.9824\n",
            "Epoch 11/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1360 - accuracy: 0.9536 - precision: 0.9269 - recall: 0.9820 - val_loss: 0.1939 - val_accuracy: 0.9240 - val_precision: 0.9280 - val_recall: 0.9823\n",
            "Epoch 12/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1363 - accuracy: 0.9548 - precision: 0.9290 - recall: 0.9827 - val_loss: 0.1772 - val_accuracy: 0.9383 - val_precision: 0.9302 - val_recall: 0.9828\n",
            "Epoch 13/20\n",
            "66/66 [==============================] - 20s 311ms/step - loss: 0.1248 - accuracy: 0.9550 - precision: 0.9312 - recall: 0.9830 - val_loss: 0.2671 - val_accuracy: 0.9103 - val_precision: 0.9319 - val_recall: 0.9832\n",
            "Epoch 14/20\n",
            "66/66 [==============================] - 20s 308ms/step - loss: 0.1239 - accuracy: 0.9591 - precision: 0.9326 - recall: 0.9834 - val_loss: 0.1887 - val_accuracy: 0.9311 - val_precision: 0.9335 - val_recall: 0.9837\n",
            "Epoch 15/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1204 - accuracy: 0.9586 - precision: 0.9343 - recall: 0.9839 - val_loss: 0.1629 - val_accuracy: 0.9433 - val_precision: 0.9353 - val_recall: 0.9838\n",
            "Epoch 16/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1096 - accuracy: 0.9641 - precision: 0.9364 - recall: 0.9837 - val_loss: 0.1992 - val_accuracy: 0.9218 - val_precision: 0.9371 - val_recall: 0.9840\n",
            "Epoch 17/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1166 - accuracy: 0.9617 - precision: 0.9376 - recall: 0.9842 - val_loss: 0.2121 - val_accuracy: 0.9261 - val_precision: 0.9382 - val_recall: 0.9844\n",
            "Epoch 18/20\n",
            "66/66 [==============================] - 20s 311ms/step - loss: 0.1111 - accuracy: 0.9641 - precision: 0.9387 - recall: 0.9846 - val_loss: 0.1604 - val_accuracy: 0.9491 - val_precision: 0.9396 - val_recall: 0.9845\n",
            "Epoch 19/20\n",
            "66/66 [==============================] - 21s 312ms/step - loss: 0.1142 - accuracy: 0.9651 - precision: 0.9404 - recall: 0.9845 - val_loss: 0.1326 - val_accuracy: 0.9541 - val_precision: 0.9412 - val_recall: 0.9846\n",
            "Epoch 20/20\n",
            "66/66 [==============================] - 20s 310ms/step - loss: 0.1042 - accuracy: 0.9665 - precision: 0.9418 - recall: 0.9847 - val_loss: 0.1457 - val_accuracy: 0.9570 - val_precision: 0.9425 - val_recall: 0.9849\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3083368210>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RDkTC0rzXl7",
        "outputId": "da4d268f-c151-4cb6-c4f6-51c027bf0ca0"
      },
      "source": [
        "# get the loss and metrics\n",
        "result = model.evaluate(X_test, y_test)\n",
        "# extract those\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "precision = result[2]\n",
        "recall = result[3]\n",
        "\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\")\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1457 - accuracy: 0.9570 - precision: 0.9427 - recall: 0.9850\n",
            "[+] Accuracy: 95.70%\n",
            "[+] Precision:   94.27%\n",
            "[+] Recall:   98.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UXANmU2zAAY"
      },
      "source": [
        "def get_predictions(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model.predict(sequence)[0]\n",
        "    # one-hot encoded vector, revert using np.argmax\n",
        "    return int2label[np.argmax(prediction)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp_xygByzbsm",
        "outputId": "f0a64b14-ebec-4500-b357-8db1ed3e9175"
      },
      "source": [
        "text = \"Congrats! double your mins txts on orange or 1 2 price linerental motorola and sonyericsson with b tooth free nokia free call. Congrats again!\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBUeLq9hzdUL",
        "outputId": "a64b9496-2938-42c1-e1ea-6a55cfce2097"
      },
      "source": [
        "text = \"Hi man, I was wondering if we can meet tomorrow.\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI5-eu1j7w4B"
      },
      "source": [
        "ref: https://www.thepythoncode.com/article/build-spam-classifier-keras-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXpd2lB7Mpu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}